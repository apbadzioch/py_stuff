{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Insights Into Texts",
   "id": "f0deb41e07d07a10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import and Preprocess Text Data",
   "id": "e3ebc3db7a19f391"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Text sourced from <a href=\"http://www.gutenberg.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Project Gutenberg</a>.\n",
    "Import the text, convert it to lowercase, and name it `text` using the chosen novel:\n",
    "```python\n",
    "text = open(\"______.txt\", encoding='utf-8').read().lower()\n",
    "```"
   ],
   "id": "7b5bd44b1ccdb3c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T06:21:27.495578Z",
     "start_time": "2025-11-08T06:21:27.484187Z"
    }
   },
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:21:36.606452Z",
     "start_time": "2025-11-08T06:21:36.598476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import text of choice\n",
    "text = open(\"alice_in_wonderland.txt\", encoding=\"utf-8\").read().lower()"
   ],
   "id": "94adc95fbf48f865",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- call the function <code>word_sentence_tokenize()</code> with your text as an argument to sentence tokenize the text and then word tokenize each sentence, returning a list of word tokenized sentences",
   "id": "8684753526c8f690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:00.550008Z",
     "start_time": "2025-11-08T06:27:00.291529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sentence and word tokenize text\n",
    "word_tokenized_text = word_sentence_tokenize(text)"
   ],
   "id": "997378718f83892c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- save any word tokenized sentence in word_tokenized_sentence to any variable and print to visualize what has been done.",
   "id": "32cd5ac1be14d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:05.378269Z",
     "start_time": "2025-11-08T06:27:05.374636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# store and print any word tokenized sentence\n",
    "single_word_tokenized_sentence = word_tokenized_text[100]\n",
    "print(single_word_tokenized_sentence)"
   ],
   "id": "bd925286c0bad00f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'shall', 'never', 'get', 'to', 'twenty', 'at', 'that', 'rate', '!']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part-of-Speech Tagging",
   "id": "a1a0d5e73d6d31b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- create a list that will hold each part-of-speech tagged sentence from the novel. this allows for syntax parsing.",
   "id": "7a1128df1dca3d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:16.046792Z",
     "start_time": "2025-11-08T06:27:16.043086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a list to hold part-of-speech tagged sentences\n",
    "pos_tagged_text = []"
   ],
   "id": "2ec441ac4368e594",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- loop through each word tokenized sentence in word_tokenized_text and POS tag each sentence using NLTKs pos_tag() function. Then append the result to the created list above.",
   "id": "2f199a2582e418b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:23.525291Z",
     "start_time": "2025-11-08T06:27:22.328437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a for loop through each word tokenized sentence\n",
    "for word_tokenized_sentence in word_tokenized_text:\n",
    "    # part-of-speech tag each sentence and append to list above\n",
    "    pos_tagged_text.append(pos_tag(word_tokenized_sentence))"
   ],
   "id": "81a5f7b256fa4c33",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- save any sentence to a variable and print to visualize what has been done.",
   "id": "74cd994c42fcaee1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:30.091574Z",
     "start_time": "2025-11-08T06:27:30.088144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# store and print any POS tagged sentence\n",
    "single_sentence = pos_tagged_text[105]\n",
    "print(single_sentence)"
   ],
   "id": "e4033e3acf59e8dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('“', 'VB'), ('how', 'WRB'), ('cheerfully', 'RB'), ('he', 'PRP'), ('seems', 'VBZ'), ('to', 'TO'), ('grin', 'VB'), (',', ','), ('how', 'WRB'), ('neatly', 'RB'), ('spread', 'VB'), ('his', 'PRP$'), ('claws', 'NN'), (',', ','), ('and', 'CC'), ('welcome', 'JJ'), ('little', 'JJ'), ('fishes', 'NNS'), ('in', 'IN'), ('with', 'IN'), ('gently', 'RB'), ('smiling', 'VBG'), ('jaws', 'NN'), ('!', '.'), ('”', 'JJ'), ('“', 'NN'), ('i', 'NN'), ('’', 'VBP'), ('m', 'JJ'), ('sure', 'JJ'), ('those', 'DT'), ('are', 'VBP'), ('not', 'RB'), ('the', 'DT'), ('right', 'NN'), ('words', 'NNS'), (',', ','), ('”', 'NNP'), ('said', 'VBD'), ('poor', 'JJ'), ('alice', 'NN'), (',', ','), ('and', 'CC'), ('her', 'PRP$'), ('eyes', 'NNS'), ('filled', 'VBN'), ('with', 'IN'), ('tears', 'NNS'), ('again', 'RB'), ('as', 'IN'), ('she', 'PRP'), ('went', 'VBD'), ('on', 'IN'), (',', ','), ('“', 'FW'), ('i', 'NN'), ('must', 'MD'), ('be', 'VB'), ('mabel', 'VBN'), ('after', 'IN'), ('all', 'DT'), (',', ','), ('and', 'CC'), ('i', 'VB'), ('shall', 'MD'), ('have', 'VB'), ('to', 'TO'), ('go', 'VB'), ('and', 'CC'), ('live', 'VB'), ('in', 'IN'), ('that', 'DT'), ('poky', 'JJ'), ('little', 'JJ'), ('house', 'NN'), (',', ','), ('and', 'CC'), ('have', 'VBP'), ('next', 'VBN'), ('to', 'TO'), ('no', 'DT'), ('toys', 'NN'), ('to', 'TO'), ('play', 'VB'), ('with', 'IN'), (',', ','), ('and', 'CC'), ('oh', 'UH'), ('!', '.')]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chunk Sentences",
   "id": "c016d84039de0be2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- \"syntax parsing\": define a piece of chunk grammar that will chunk a noun phrase, consisting of an optional determiner <code>DT</code>, followed by any number of adjectives <code>JJ</code>, followed by a noun <code>NN</code>",
   "id": "9d4482d543d5fa6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:39.597722Z",
     "start_time": "2025-11-08T06:27:39.594602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define noun phrase chunk grammar\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ],
   "id": "b74e7ca23a9b3b70",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- create a <code>nltk RegexpParser</code> object using the noun phrase chunk grammar defined above as an argument",
   "id": "fff1166c28a81956"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T06:27:47.657845Z",
     "start_time": "2025-11-08T06:27:47.649333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create noun phrase object\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ],
   "id": "22e0222c49b96bd1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f36291223f7c3bcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
